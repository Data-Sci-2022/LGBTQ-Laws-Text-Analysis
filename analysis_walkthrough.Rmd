---
title: "LGBTQ Laws Text Analysis"
author: "Mack Campbell"
date: "December 15, 2022"
output: 
  github_document:
    toc: TRUE 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, include = TRUE, comment = NA, fig.path = 'data_visualization/')
library(tidyverse)
library(rvest)
library(pdftools)
library(tidytext)
```

# Creating State Dataframe

```{r state-abbrs}
# Equaldex and MAP have data on more than the 50 states, so this will be used to filter and cross-reference state name and abbreviations.
States <- tibble(State = state.name, State.Abbr = state.abb)
```

# Web Scraping / Data Collection

## Table Scraping and Manipulation

### Equaldex

[Equaldex](https://www.equaldex.com/equality-index/united-states) measures legal protections and public opinions of those in the LGBTQ+ community to come up with an overall equality index per state.

```{r equaldex-scrape}
Equaldex_url <- read_html("https://www.equaldex.com/equality-index/united-states")

# extracting the raw data
Equaldex_raw <- Equaldex_url %>% 
  html_element(xpath ='//*[@id="content"]/div[3]/table') %>% 
  html_table()
```

```{r equaldex-manipulation}
# extracting and manipulating the data
# index is on a scale out of 100
Equaldex_df <- Equaldex_url %>% 
  html_element(xpath ='//*[@id="content"]/div[3]/table') %>% 
  html_table() %>% 
  # separating column data
  separate('State and Territory', into = c('Rank', 'State'), sep = "\\. ", convert = T) %>% 
  separate('Equality Index', into = c('Equality_Index', 'Max1'), convert = T) %>% 
  separate('Legal Index', into = c('Legal_Index', 'Max2'), convert = T) %>% 
  separate('Public Opinion Index', into = c('Public_Opinion_Index', 'Max3'), convert = T) %>% 
  #removing unnecessary columns 
  select(-c(Max1, Max2, Max3)) %>%  
  semi_join(States, by = 'State')

head(Equaldex_df)
```

### MAP

[Movement Advancement Project (MAP)](https://www.lgbtmap.org/equality-maps) measures seven different aspects of state protections for gender identity (GI) and sexual orientation (SO).

```{r MAP-scrape}
MAP_url <- read_html("https://www.lgbtmap.org/equality-maps")

# extracting the raw data
MAP_raw <- MAP_url %>% 
  html_element(xpath = '//*[@id="map-4"]/div/table') %>% 
  html_table()
```

```{r MAP-manipulation1}
# extracting and manipulating
MAP_df <- MAP_url %>% 
  html_element(xpath = '//*[@id="map-4"]/div/table') %>% 
  html_table(header = T) %>% # works to rename, how to make it proper titles, how to add missing titles?
  rename('Alpha_Rank' = 1, 'Measured_Category' = 3) %>%  #renaming empty columns
  select(-Alpha_Rank) %>% #removes unnecessary initial column that numbered state positions
  semi_join(States, by = 'State') %>% 
  #separating the score and the scale
  separate('Relationship andParental Recognition', into = c('Relationship_and_Parental_Recognition', 'Relationship_and_Parental_Recognition_Scale'), sep = '/', convert = T) %>% 
  separate('Non–Discrimination', into = c('Non-Discrimination', 'Non-Discrimination_Scale'), sep = '/', convert = T) %>% 
  separate('Religious Exemption Laws', into = c('Religious_Exemption_Laws', 'Religious_Exemption_Laws_Scale'), sep = '/', convert = T) %>% 
  separate('LGBT Youth', into = c('LGBT_Youth', 'LGBT_Youth_Scale'), sep = '/', convert = T) %>% 
  separate('Healthcare', into = c('Healthcare', 'Healthcare_Scale'), sep = '/', convert = T) %>% 
  separate('CriminalJustice', into = c('Criminal_Justice', 'Criminal_Justice_Scale'), sep = '/', convert = T) %>% 
  separate('IdentityDocuments', into = c('Identity_Documents', 'Identity_Documents_Scale'), sep = '/', convert = T) %>% 
  rename('Sexual_Orientation_Policy_Tally' = 'Sexual Orientation Policy Tally', 'Gender_Identity_Policy_Tally' = 'Gender Identity Policy Tally', 'Overall_Tally'= 'Overall Tally')
```


```{r MAP-manipulation2}
# Pivoting wider to separate gender identity and sexual orientation and cleaning up columns.
# vector types are a mess in the columns
MAP_df <- MAP_df %>% 
  pivot_wider(names_from = Measured_Category, names_sep = "_", values_from = Relationship_and_Parental_Recognition:Identity_Documents_Scale) %>% 
  select(-c(Identity_Documents_SO, Identity_Documents_Scale_SO)) %>%  # removing unnecessary columns
  # rearrange columns
  relocate('State', 'Relationship_and_Parental_Recognition_GI', 'Relationship_and_Parental_Recognition_Scale_GI','Relationship_and_Parental_Recognition_SO', 'Relationship_and_Parental_Recognition_Scale_SO', 'Non-Discrimination_GI', 'Non-Discrimination_Scale_GI', 'Non-Discrimination_SO', 'Non-Discrimination_Scale_SO', 'Religious_Exemption_Laws_GI', 'Religious_Exemption_Laws_Scale_GI','Religious_Exemption_Laws_SO', 'Religious_Exemption_Laws_Scale_SO', 'LGBT_Youth_GI', 'LGBT_Youth_Scale_GI', 'LGBT_Youth_SO', 'LGBT_Youth_Scale_SO', 'Healthcare_GI', 'Healthcare_Scale_GI', 'Healthcare_SO', 'Healthcare_Scale_SO', 'Criminal_Justice_GI', 'Criminal_Justice_Scale_GI', 'Criminal_Justice_SO', 'Criminal_Justice_Scale_SO', 'Identity_Documents_GI', 'Identity_Documents_Scale_GI', ends_with('Tally'))
head(MAP_df)
```

## Freedom for All Americans

The Freedom for All Americans' website uses a widget for the law data I am looking for, so I ran a JavaScript code that pulled all necessary information and saved it as separate CSV files.

/*
	Download State Tables from https://freedomforallamericans.org/legislative-tracker/
	
	To use:
	1. Load https://freedomforallamericans.org/legislative-tracker/
	2. Copy & paste the entire script text into your browser's console
	3. Click a state on the map. In the console, hit <enter>/<return> to run this code
	4. Click another state, use the up arrow to load this script text again, rinse, and repeat
​
	90% of this code comes from https://www.geeksforgeeks.org/how-to-export-html-table-to-csv-using-javascript/
	Adapted by Dan Villarreal

```{}
​
function tableToCSV(node) {
	// State name
	var sName = node.querySelector(".grid-head").innerText;
	var end = sName.search(/ \(/g);
	var sName = sName.slice(0, end);
	
	// Variable to store the final csv data
	var csv_data = [];
​
	// Get each row data
	var rows = node.querySelectorAll('.tableWrapper > table tr');
	for (var i = 0; i < rows.length; i++) {
​
		// Get each column data
		var cols = rows[i].querySelectorAll('td,th');
​
		// Stores each csv row data
		var csvrow = [];
		for (var j = 0; j < cols.length; j++) {
			// Get data from each cell & push it to csvrow
			// First cell in first row: "Link"
			if (i == 0 && j == 0) {
				csvrow.push('Link')
			// First column: get link
			} else if (j == 0) {
				csvrow.push(cols[j].querySelector('a').href);				
			// Otherwise: get text
			} else {
				csvrow.push('"' + cols[j].innerText + '"');				
			} 
		}
​
		// Combine each column value with comma
		csv_data.push(csvrow.join(","));
	}
​
	// Combine each row data with new line character
	csv_data = csv_data.join('\n');
​
	// Call this function to download csv file
	downloadCSVFile(csv_data, sName);
}
​
function downloadCSVFile(csv_data, csv_name) {
​
	// Create CSV file object and feed
	// our csv_data into it
	CSVFile = new Blob([csv_data], {
		type: "text/csv"
	});
​
	// Create to temporary link to initiate
	// download process
	var temp_link = document.createElement('a');
​
	// Download csv file
	temp_link.download = csv_name;
	var url = window.URL.createObjectURL(CSVFile);
	temp_link.href = url;
​
	// This link should not be displayed
	temp_link.style.display = "none";
	document.body.appendChild(temp_link);
​
	// Automatically click the link to
	// trigger download
	temp_link.click();
	document.body.removeChild(temp_link);
}
​
var node = document.querySelector("#BT50MapWidget");
tableToCSV(node);
```

Read in all files as a character vector.

```{r loading-files}
# pro
pro_files <- list.files(path = 'data/State-Bill-csvs/Pro/', pattern = '.csv', full.names = T)

# anti
anti_files <- list.files(path = 'data/State-Bill-csvs/Anti/', pattern = '.csv', full.names = T)
```

And map over them to convert them into tibbles.

```{r pro-laws-tibble, message=FALSE}
pro_laws <- pro_files %>% 
  map_dfr(., read_csv, name_repair = 'universal') %>% 
  # drop position, it is a blank column
  select(-c(Position)) %>% 
  rename('BillTrack.Link' = 'Link', 'State.Abbr' = 'State') %>%  
  mutate('Action.Year' = str_extract(Action.Date, '\\d\\d\\d\\d$'), .after = 'Action.Date') %>% 
  mutate('Bill.Stance' = 'Pro', .after = BillTrack.Link)
head(pro_laws)
```

```{r anti-laws-tibble, message=FALSE}
anti_laws <- anti_files %>% 
  map_dfr(., read_csv, name_repair = 'universal') %>% 
  # drop position, it is a blank column
  select(-c(Position)) %>% 
  rename('BillTrack.Link' = 'Link', 'State.Abbr' = 'State') %>%  
  mutate('Action.Year' = str_extract(Action.Date, '\\d\\d\\d\\d$'), .after = 'Action.Date')%>% 
  mutate('Bill.Stance' = 'Anti', .after = BillTrack.Link)
head(anti_laws)
```

Join them together and add full state name.

```{r all-laws-tibble}
all_laws <- full_join(anti_laws, pro_laws) %>% 
  left_join(States, by = 'State.Abbr') %>% 
  relocate(State, .after = State.Abbr)
head(all_laws)
```

## BillTrack50

The code below is how I used `rvest` to pull necessary information from [BillTrack50](https://www.billtrack50.com/). I got explicit permission to scrape this data, but their TOS prohibits scraping unless otherwise approved.

```{r billtrack-urls, eval=FALSE}
# create a character vector of all urls to pull data from
billtrack_urls <- c(anti_laws$BillTrack.Link, pro_laws$BillTrack.Link)
```

It is too big to run as one, so I will chunk it up.

```{r billtrack-scrape, eval=FALSE}
billtrack_df1 <- tibble('BillTrack.Link' = billtrack_urls[1:95]) %>% 
  mutate('Bill.PDF' = BillTrack.Link %>% map(~ .x %>% 
                                          read_html() %>% 
                                          # the links to each state information is housed in a table
                                          html_element('#docs-table') %>% 
                                          # returns a character vector/list in each column
                                          html_table() %>% 
                                            filter(grepl('Bill(Text)?', `Document Type`), 
                                                   grepl('pdf', `Source Location`, ignore.case = T)) %>% 
                                          pull(`Source Location`))) 

billtrack_df2 <- tibble('BillTrack.Link' = billtrack_urls[96:190]) %>% 
  mutate('Bill.PDF' = BillTrack.Link %>% map(~ .x %>% 
                                          read_html() %>% 
                                          html_element('#docs-table') %>% 
                                          html_table() %>% 
                                            filter(grepl('Bill(Text)?', `Document Type`), 
                                                   grepl('pdf', `Source Location`, ignore.case = T)) %>% 
                                          pull(`Source Location`)))

billtrack_df3 <- tibble('BillTrack.Link' = billtrack_urls[191:285]) %>% 
  mutate('Bill.PDF' = BillTrack.Link %>% map(~ .x %>% 
                                          read_html() %>% 
                                          html_element('#docs-table') %>% 
                                          html_table() %>% 
                                            filter(grepl('Bill(Text)?', `Document Type`), 
                                                   grepl('pdf', `Source Location`, ignore.case = T)) %>% 
                                          pull(`Source Location`)))
```

Join them into 1 df.

```{r billtrack-tibble, eval=FALSE}
# creating one tibble
billtrack_df <- tibble('BillTrack.Link' = c(billtrack_df1$BillTrack.Link, billtrack_df2$BillTrack.Link, billtrack_df3$BillTrack.Link), 'State.Link' = c(billtrack_df1$Bill.PDF, billtrack_df2$Bill.PDF, billtrack_df3$Bill.PDF))
```

Unnest individual URLs, then filter for only PDFs.

```{r billtrack-tibble-manipulation, eval=FALSE}
billtrack_df <- billtrack_df %>% 
  # generates a row per URL
  unnest(cols = State.Link, keep_empty = T) %>% 
  # some state URLs are listed twice, remove duplicates
  filter(!duplicated(State.Link))
billtrack_df
```

Save the tibble as a csv to prevent future scraping.

```{r billtrack-tibble-to-csv, eval=FALSE}
# saving the tibble into a csv to avoid future scraping
billtrack_df %>% 
  write.csv('data/billtrackdf.csv', row.names = T)
```

Read in the csv instead of the tibble created above.

```{r read-billtrack-csv}
# read in billtrack_df without scraping
if (!exists("billtrack_df")) {
  billtrack_df <- read_csv('data/billtrackdf.csv')
}
```

I tested one link from each state to verify that the links worked.

```{r state-pdf}
# combine links and state info
state_pdf_df <- billtrack_df %>% 
  left_join(all_laws, by = 'BillTrack.Link') %>% 
  select(BillTrack.Link, Bill.Stance, State.Link, State) %>% 
  # Delaware returned an NA
  filter(!is.na(State.Link)) %>% 
  # indiana links generate a 503 error that could not be resolved
  # south dakota pdfs could not be read by pdf_text or pdf_ocr_text
  # washington links generated a url error
  filter(!State %in% c('Indiana', 'South Dakota', 'Washington')) %>% 
  # some non-pdfs slipped through
  filter(grepl('pdf$', State.Link, ignore.case = T)) %>% 
  # links for Virginia are formatted incorrectly (contains the string "+hil")
  mutate(State.Link = State.Link %>% map_chr( ~.x %>% 
    str_remove('\\+hil')))
```

# Working with PDFs

Instructions pulled from: https://data.library.virginia.edu/reading-pdf-files-into-r-for-text-mining/

```{r pdf-clean-function}
# create a function to extract and tidy up the pdfs
clean_pdf <- function(file) {
  file %>% 
    pdf_text() %>% 
    str_split("[\\r\\n]+") %>%
    # remove pagination
    flatten_chr() %>%  
    # trim leading white space
    str_trim() %>% 
    # remove inner white space
    str_squish() %>% 
    # remove line numbers
    str_remove("^\\d+")
  }  
```

The process was error-laden, so I broke the links up into small chunks to be able to diagnose issues easier.

```{r pdf-gathering, eval=FALSE}
# running df through the extracting and cleaning function
state_text_df1 <- tibble('BillTrack.Link' = state_pdf_df$BillTrack.Link[1:50], 'Bill.Stance' = state_pdf_df$Bill.Stance[1:50], 'State.Link' = state_pdf_df$State.Link[1:50], 'State' = state_pdf_df$State[1:50]) %>% 
  mutate('Bill.Text' = map(State.Link, clean_pdf))

state_text_df2 <- tibble('BillTrack.Link' = state_pdf_df$BillTrack.Link[51:100], 'Bill.Stance' = state_pdf_df$Bill.Stance[51:100], 'State.Link' = state_pdf_df$State.Link[51:100], 'State' = state_pdf_df$State[51:100]) %>% 
  mutate('Bill.Text' = map(State.Link, clean_pdf))

state_text_df3 <- tibble('BillTrack.Link' = state_pdf_df$BillTrack.Link[101:150], 'Bill.Stance' = state_pdf_df$Bill.Stance[101:150], 'State.Link' = state_pdf_df$State.Link[101:150], 'State' = state_pdf_df$State[101:150]) %>% 
  mutate('Bill.Text' = map(State.Link, clean_pdf))

state_text_df4 <- tibble('BillTrack.Link' = state_pdf_df$BillTrack.Link[151:200], 'Bill.Stance' = state_pdf_df$Bill.Stance[151:200], 'State.Link' = state_pdf_df$State.Link[151:200], 'State' = state_pdf_df$State[151:200]) %>% 
  mutate('Bill.Text' = map(State.Link, clean_pdf))

state_text_df5 <- tibble('BillTrack.Link' = state_pdf_df$BillTrack.Link[201:250], 'Bill.Stance' = state_pdf_df$Bill.Stance[201:250], 'State.Link' = state_pdf_df$State.Link[201:250], 'State' = state_pdf_df$State[201:250]) %>% 
  mutate('Bill.Text' = map(State.Link, clean_pdf))

state_text_df6<- tibble('BillTrack.Link' = state_pdf_df$BillTrack.Link[251:300], 'Bill.Stance' = state_pdf_df$Bill.Stance[251:300], 'State.Link' = state_pdf_df$State.Link[251:300], 'State' = state_pdf_df$State[251:300]) %>% 
  mutate('Bill.Text' = map(State.Link, clean_pdf))

state_text_df7 <- tibble('BillTrack.Link' = state_pdf_df$BillTrack.Link[301:350], 'Bill.Stance' = state_pdf_df$Bill.Stance[301:350], 'State.Link' = state_pdf_df$State.Link[301:350], 'State' = state_pdf_df$State[301:350]) %>% 
  mutate('Bill.Text' = map(State.Link, clean_pdf))

state_text_df8 <- tibble('BillTrack.Link' = state_pdf_df$BillTrack.Link[351:363], 'Bill.Stance' = state_pdf_df$Bill.Stance[351:363], 'State.Link' = state_pdf_df$State.Link[351:363], 'State' = state_pdf_df$State[351:363]) %>% 
  mutate('Bill.Text' = map(State.Link, clean_pdf))
```

Combine into one tibble.

```{r state-bill-tibble, eval=FALSE}
# creating one tibble
state_text_df <- tibble('BillTrack.Link' = c(state_text_df1$BillTrack.Link, state_text_df2$BillTrack.Link, state_text_df3$BillTrack.Link, state_text_df4$BillTrack.Link, state_text_df5$BillTrack.Link, state_text_df6$BillTrack.Link, state_text_df7$BillTrack.Link, state_text_df8$BillTrack.Link), 
                        'Bill.Stance' = c(state_text_df1$Bill.Stance, state_text_df2$Bill.Stance, state_text_df3$Bill.Stance, state_text_df4$Bill.Stance, state_text_df5$Bill.Stance, state_text_df6$Bill.Stance, state_text_df7$Bill.Stance, state_text_df8$Bill.Stance), 
                        'State.Link' = c(state_text_df1$State.Link, state_text_df2$State.Link, state_text_df3$State.Link, state_text_df4$State.Link, state_text_df5$State.Link, state_text_df6$State.Link, state_text_df7$State.Link, state_text_df8$State.Link), 
                        'State' = c(state_text_df1$State, state_text_df2$State, state_text_df3$State, state_text_df4$State, state_text_df5$State, state_text_df6$State, state_text_df7$State, state_text_df8$State), 
                        'Bill.Text' = c(state_text_df1$Bill.Text, state_text_df2$Bill.Text, state_text_df3$Bill.Text, state_text_df4$Bill.Text, state_text_df5$Bill.Text, state_text_df6$Bill.Text, state_text_df7$Bill.Text, state_text_df8$Bill.Text))
```

# Text Analysis

## Tokenization

```{r unnesting-text, eval=FALSE}
# Bill.Text is lines of text per legislation.
state_text_df <- state_text_df %>% 
  unnest(Bill.Text)
```

```{r state-text-tibble-to-csv, eval=FALSE}
# saving the tibble into a csv to avoid future scraping
state_text_df %>% 
  write.csv('data/statetextdf.csv', row.names = T)
```

```{r read-state-text-tibble-csv}
# read in state_text_df without scraping
if (!exists("state_text_df")) {
  state_text_df <- read_csv('data/statetextdf.csv')
}
```

```{r focus-words}
# based on top word types and focused for analysis
focus_words <- tibble(word = c("gender", "sex", "orientation", "identity", "transgender", "education", "intermediate", "school", "student", "students", "pupil", "pupils", "child", "children"))
```

Tokenizing the text into unigrams, bigrams, and trigrams.

```{r tokenizing-text}
# unigrams
state_text_df_unigram <- state_text_df %>% 
  unnest_tokens(Bill1gram, Bill.Text, token = "words", to_lower = T, drop = F) %>% 
  # filtering out words not in focus list
  semi_join(focus_words, by = c("Bill1gram" = "word"))

# bigrams
state_text_df_bigram <- state_text_df %>% 
  unnest_tokens(Bill2gram, Bill.Text, token = "ngrams", n = 2, to_lower = T, drop = F) %>% 
  # adding in unigrams to filter on focus words
  unnest_tokens(Unigram, Bill2gram, drop = F) %>% 
  semi_join(focus_words, by = c("Unigram" = "word"))

# trigrams
state_text_df_trigram <- state_text_df %>% 
  unnest_tokens(Bill3gram, Bill.Text, token = "ngrams", n = 3, to_lower = T, drop = F) %>% 
  # adding in unigrams to filter on focus words
  unnest_tokens(Unigram, Bill3gram, drop = F) %>% 
  semi_join(focus_words, by = c("Unigram" = "word"))
```

Looking at n-gram frequencies.

```{r unigram-frequency}
# mean token frequency
state_text_df_unigram %>% 
  filter(Bill.Stance == 'Anti') %>% 
  group_by(Bill1gram) %>% 
  summarize(Token_mean = length(Bill1gram)/n_distinct(State.Link)) %>% 
  arrange(desc(Token_mean)) %>% 
  head()

state_text_df_unigram %>% 
  filter(Bill.Stance == 'Pro') %>% 
  group_by(Bill1gram) %>% 
  summarize(Token_mean = length(Bill1gram)/n_distinct(State.Link)) %>% 
  arrange(desc(Token_mean)) %>% 
  head()

# raw token frequency
state_text_df_unigram %>% 
  filter(Bill.Stance == 'Anti') %>% 
  count(Bill1gram, sort = T, name = 'Token_Count') %>% 
  head()

state_text_df_unigram %>% 
  filter(Bill.Stance == 'Pro') %>% 
  count(Bill1gram, sort = T, name = 'Token_Count') %>% 
  head()
```


```{r bigram-frequency}
# mean token frequency
state_text_df_bigram %>% 
  filter(Bill.Stance == 'Anti') %>% 
  group_by(Bill2gram) %>% 
  summarize(Token_mean = length(Bill2gram)/n_distinct(State.Link)) %>% 
  arrange(desc(Token_mean)) %>% 
  head()

state_text_df_bigram %>% 
  filter(Bill.Stance == 'Pro') %>% 
  group_by(Bill2gram) %>% 
  summarize(Token_mean = length(Bill2gram)/n_distinct(State.Link)) %>% 
  arrange(desc(Token_mean)) %>% 
  head()

# raw token frequency
state_text_df_bigram %>% 
  filter(Bill.Stance == 'Anti') %>% 
  count(Bill2gram, sort = T, name = 'Token_Count') %>% 
  head()

state_text_df_bigram %>% 
  filter(Bill.Stance == 'Pro') %>% 
  count(Bill2gram, sort = T, name = 'Token_Count') %>% 
  head()
```


```{r trigram-frequency}
# mean token frequency
state_text_df_trigram %>% 
  filter(Bill.Stance == 'Anti') %>% 
  group_by(Bill3gram) %>% 
  summarize(Token_mean = length(Bill3gram)/n_distinct(State.Link)) %>% 
  arrange(desc(Token_mean)) %>% 
  head()

state_text_df_trigram %>% 
  filter(Bill.Stance == 'Pro') %>% 
  group_by(Bill3gram) %>% 
  summarize(Token_mean = length(Bill3gram)/n_distinct(State.Link)) %>% 
  arrange(desc(Token_mean)) %>% 
  head()

# raw token frequency
state_text_df_trigram %>% 
  filter(Bill.Stance == 'Anti') %>% 
  count(Bill3gram, sort = T, name = 'Token_Count') %>% 
  head()

state_text_df_trigram %>% 
  filter(Bill.Stance == 'Pro') %>% 
  count(Bill3gram, sort = T, name = 'Token_Count') %>% 
  head()
```

# Correlation

```{r unigram-tabulation}
State_gs_count<- state_text_df_unigram %>% 
  filter(Bill1gram %in% c("gender", "sex")) %>% 
  group_by(State, Bill1gram) %>% 
  summarize(Token_mean = length(Bill1gram)/n_distinct(State.Link), .groups = 'drop_last') %>% 
  pivot_wider(names_from = Bill1gram, values_from = Token_mean, values_fill = 0) %>% 
  rename("Gender.Token.Mean" = gender, "Sex.Token.Mean" = sex)
head(State_gs_count)
```


```{r legislation-and-ranks}
#state rank, number of legislation
correlation_df<- all_laws %>% 
  group_by(State, Bill.Stance) %>% 
  summarize(n = length(Bill.Number), .groups = 'drop_last') %>% 
  # widen to show anti and pro side by side per state
  pivot_wider(names_from = "Bill.Stance", values_from = 'n', values_fill = 0) %>% 
  # renaming for clarification
  rename('Anti.Laws' = Anti, 'Pro.Laws' = Pro) %>% 
  # adding total legislation
  mutate('All.Laws' = Anti.Laws + Pro.Laws) %>% 
  # adding Equaldex data
  left_join(Equaldex_df, by = 'State') %>% 
  # adding MAP data
  left_join(MAP_df, by = 'State') %>% 
  # adding unigram data
  left_join(State_gs_count, by = 'State') %>% 
  # selecting relevant columns to compare
  select(c('Anti.Laws', 'Pro.Laws', 'All.Laws', 'Gender.Token.Mean', 'Sex.Token.Mean', 'Rank', 'Equality_Index', 'Overall_Tally')) %>% 
  # renaming for clarification
  rename('Equaldex_Index' = Equality_Index, 'MAP_Tally' = Overall_Tally, 'Equaldex_Rank' = Rank) %>% 
  # filtering out NAs
  filter(!is.na(Gender.Token.Mean))
head(correlation_df)
```

```{r correlation}
# MAP and Equaldex correlation
cor(correlation_df$Equaldex_Index, correlation_df$MAP_Tally)

# test for correlation between # of laws and rank
cor(correlation_df$All.Laws, correlation_df$Equaldex_Index)

# test for correlation between # of anti laws and rank
cor(correlation_df$Anti.Laws, correlation_df$Equaldex_Index)

# test for correlation between # of pro laws and rank
cor(correlation_df$Pro.Laws, correlation_df$Equaldex_Index)

# sex correlation
cor(correlation_df$Sex.Token.Mean, correlation_df$Equaldex_Index)

# gender correlation
cor(correlation_df$Gender.Token.Mean, correlation_df$Equaldex_Index)
```

```{r rank-and-law-correlation}
correlation_df %>% 
  ggplot(aes(x = correlation_df$All.Laws, y = correlation_df$Equaldex_Index, label = correlation_df$State)) +
  geom_point(alpha = 0.3) +
  geom_text(size = 3) +
  geom_smooth(method = 'lm', formula = y ~ x, alpha = 0.2) +
  labs(x = 'Current Legislation', y = 'Equaldex Index') +
  ggtitle("State Equaldex Index and Current Legislation") +
  theme_gray()
```

```{r rank-and-sex-correlation}
correlation_df %>% 
  ggplot(aes(x = correlation_df$Sex.Token.Mean, y = correlation_df$Equaldex_Index, label = correlation_df$State)) +
  geom_point(alpha = 0.3) +
  geom_text(size = 3) +
  geom_smooth(method = 'lm', formula = y ~ x, alpha = 0.2) +
  labs(x = 'Sex Mean Token Usage', y = 'Equaldex Index') +
  ggtitle("State Equaldex Index and Sex Mean Token Usage") +
  theme_gray()
```


```{r rank-and-gender-correlation}
correlation_df %>% 
  ggplot(aes(x = correlation_df$Gender.Token.Mean, y = correlation_df$Equaldex_Index, label = correlation_df$State)) +
  geom_point(alpha = 0.3) +
  geom_text(size = 3) +
  geom_smooth(method = 'lm', formula = y ~ x, alpha = 0.2) +
  labs(x = 'Gender Mean Token Usage', y = 'Equaldex Index') +
  ggtitle("State Equaldex Index and Gender Mean Token Usage") +
  theme_gray()
```

# Unigram Data Visualization

```{r unigram-plot-gender}
state_text_df_unigram %>% 
  filter(Bill1gram == "gender") %>% 
  group_by(State, Bill.Stance, Bill1gram) %>% 
  summarize(Token_mean = length(Bill1gram)/n_distinct(State.Link), .groups = 'drop_last') %>% 
  ggplot(aes(y = State, x = Token_mean, fill = Bill.Stance)) +
  geom_bar(stat = "identity") +
  ggtitle("Token Frequency: 'gender'")
```

```{r unigram-plot-sex}
state_text_df_unigram %>% 
  filter(Bill1gram == "sex") %>% 
  group_by(State, Bill.Stance, Bill1gram) %>% 
  summarize(Token_mean = length(Bill1gram)/n_distinct(State.Link), .groups = 'drop_last') %>% 
  ggplot(aes(y = State, x = Token_mean, fill = Bill.Stance)) +
  geom_bar(stat = "identity") +
  ggtitle("Token Frequency: 'sex'")
```


```{r unigram-plot-orientation}
state_text_df_unigram %>% 
  filter(Bill1gram == "orientation") %>% 
  group_by(State, Bill.Stance, Bill1gram) %>% 
  summarize(Token_mean = length(Bill1gram)/n_distinct(State.Link), .groups = 'drop_last') %>% 
  ggplot(aes(y = State, x = Token_mean, fill = Bill.Stance)) +
  geom_bar(stat = "identity") +
  ggtitle("Token Frequency: 'orientation'")
```

```{r unigram-plot-identity}
state_text_df_unigram %>% 
  filter(Bill1gram == "identity") %>% 
  group_by(State, Bill.Stance, Bill1gram) %>% 
  summarize(Token_mean = length(Bill1gram)/n_distinct(State.Link), .groups = 'drop_last') %>% 
  ggplot(aes(y = State, x = Token_mean, fill = Bill.Stance)) +
  geom_bar(stat = "identity") +
  ggtitle("Token Frequency: 'identity'")
```

```{r unigram-plot-transgender}
state_text_df_unigram %>% 
  filter(Bill1gram == "transgender") %>% 
  group_by(State, Bill.Stance, Bill1gram) %>% 
  summarize(Token_mean = length(Bill1gram)/n_distinct(State.Link), .groups = 'drop_last') %>% 
  ggplot(aes(y = State, x = Token_mean, fill = Bill.Stance)) +
  geom_bar(stat = "identity") +
  ggtitle("Token Frequency: 'transgender'")
```

```{r unigram-plot-education}
state_text_df_unigram %>% 
  filter(Bill1gram == "education") %>% 
  group_by(State, Bill.Stance, Bill1gram) %>% 
  summarize(Token_mean = length(Bill1gram)/n_distinct(State.Link), .groups = 'drop_last') %>% 
  ggplot(aes(y = State, x = Token_mean, fill = Bill.Stance)) +
  geom_bar(stat = "identity") +
  ggtitle("Token Frequency: 'education'")
```

```{r unigram-plot-school}
state_text_df_unigram %>% 
  filter(Bill1gram == "school") %>% 
  group_by(State, Bill.Stance, Bill1gram) %>% 
  summarize(Token_mean = length(Bill1gram)/n_distinct(State.Link), .groups = 'drop_last') %>% 
  ggplot(aes(y = State, x = Token_mean, fill = Bill.Stance)) +
  geom_bar(stat = "identity") +
  ggtitle("Token Frequency: 'school'")
```

```{r unigram-plot-student}
state_text_df_unigram %>% 
  filter(Bill1gram %in% c('student', 'students')) %>% 
  group_by(State, Bill.Stance, Bill1gram) %>% 
  summarize(Token_mean = length(Bill1gram)/n_distinct(State.Link), .groups = 'drop_last') %>% 
  ggplot(aes(y = State, x = Token_mean, fill = Bill.Stance)) +
  geom_bar(stat = "identity") +
  ggtitle("Token Frequency: 'student' and 'students'")
```

```{r unigram-plot-children}
state_text_df_unigram %>% 
  filter(Bill1gram %in% c('child', 'children')) %>% 
  group_by(State, Bill.Stance, Bill1gram) %>% 
  summarize(Token_mean = length(Bill1gram)/n_distinct(State.Link), .groups = 'drop_last') %>% 
  ggplot(aes(y = State, x = Token_mean, fill = Bill.Stance)) +
  geom_bar(stat = "identity") +
  ggtitle("Token Frequency: 'child' and 'children'")
```

```{r session-info}
sessionInfo()
```

