---
title: "Data Sample"
date: "2022-11-03"
output:
  github_document:
    toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r load_packages, warning=FALSE, message=FALSE}
library(tidyverse)
library(rvest)
library(pdftools)
```

```{r}
# build a simple tibble to connect state name and abbreviations, used for filtering
States <- tibble(State = state.name, Abbreviation = state.abb)
```


# Equaldex Data

Save the webpage html for easier access in following codes:

```{r save_equaldex_html}
Equaldex <- read_html("https://www.equaldex.com/equality-index/united-states")
```

Scrape the table and save as a tibble.

```{r equaldex_table}
Equaldex_raw <- Equaldex %>% 
  html_element(xpath ='//*[@id="content"]/div[3]/table') %>% 
  html_table()
head(Equaldex_raw)
```

Tidy up the data.

```{r equaldex_manip}
Equaldex_df <- Equaldex %>% 
  html_element(xpath ='//*[@id="content"]/div[3]/table') %>% 
  html_table() %>% 
  # separating column data
  separate('State and Territory', into = c('Rank', 'State'), sep = "\\. ", convert = T) %>% 
  separate('Equality Index', into = c('Equality_Index', 'Max1'), convert = T) %>% 
  separate('Legal Index', into = c('Legal_Index', 'Max2'), convert = T) %>% 
  separate('Public Opinion Index', into = c('Public_Opinion_Index', 'Max3'), convert = T) %>% 
  select(-c(Max1, Max2, Max3)) %>%  #removing unnecessary columns 
  mutate('Total_Index'= (Equality_Index + Legal_Index + Public_Opinion_Index) / 3) %>% #making average colummn to compare to MAP data
  semi_join(States, by = 'State')

head(Equaldex_df)
```


# MAP Data

Save the webpage html for easier access in following codes:

```{r save_map_html}
MAP <- read_html("https://www.lgbtmap.org/equality-maps")
```

Scrape the table and save as a tibble.

```{r map_table}
MAP_raw <- MAP %>% 
  html_element(xpath = '//*[@id="map-4"]/div/table') %>% 
  html_table()
head(MAP_raw)
```

Tidy up the data.

```{r map_manip}
MAP_df <- MAP %>% 
  html_element(xpath = '//*[@id="map-4"]/div/table') %>% 
  html_table(header = T) %>% # works to rename, how to make it proper titles, how to add missing titles?
  rename('Alpha_Rank' = 1, 'Measured_Category' = 3) %>%  #renaming empty columns
  select(-Alpha_Rank) %>% #removes unnecessary initial column that numbered state positions
  semi_join(States, by = 'State') %>% 
  #separating the score and the scale
  separate('Relationship andParental Recognition', into = c('Relationship_and_Parental_Recognition', 'Relationship_and_Parental_Recognition_Scale'), sep = '/', convert = T) %>% 
  separate('Non–Discrimination', into = c('Non-Discrimination', 'Non-Discrimination_Scale'), sep = '/', convert = T) %>% 
  separate('Religious Exemption Laws', into = c('Religious_Exemption_Laws', 'Religious_Exemption_Laws_Scale'), sep = '/', convert = T) %>% 
  separate('LGBT Youth', into = c('LGBT_Youth', 'LGBT_Youth_Scale'), sep = '/', convert = T) %>% 
  separate('Healthcare', into = c('Healthcare', 'Healthcare_Scale'), sep = '/', convert = T) %>% 
  separate('CriminalJustice', into = c('Criminal_Justice', 'Criminal_Justice_Scale'), sep = '/', convert = T) %>% 
  separate('IdentityDocuments', into = c('Identity_Documents', 'Identity_Documents_Scale'), sep = '/', convert = T)

head(MAP_df)
```


# PDF

Instructions pulled from: https://data.library.virginia.edu/reading-pdf-files-into-r-for-text-mining/

The output is rather long, so I will hide it from view, but the following code is used to generate the object.

```{r results=FALSE}
# pro legislation from nebraska
pdf_text('https://nebraskalegislature.gov/FloorDocs/107/PDF/Intro/LB120.pdf') %>% 
  str_split("[\\r\\n]+") %>% 
  map(~ .x %>% 
        str_trim() %>% # trims leading white space
        str_squish() %>% # removes inner white space
        str_remove("^\\d+ ")) # remove line numbers
```


```{r results=FALSE}
# anti from nebraska
pdf_text('https://nebraskalegislature.gov/FloorDocs/107/PDF/Intro/LB1077.pdf') %>% 
  str_split("[\\r\\n]+") %>% 
  map(~ .x %>% 
        str_trim() %>% 
        str_squish() %>% 
        str_remove("^\\d+ "))
```

